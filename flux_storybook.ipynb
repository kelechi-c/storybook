{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "from tqdm.auto import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "torch_dtype = torch.bfloat16\n",
    "device = \"cuda\"\n",
    "model_id = \"Freepik/flux.1-lite-8B-alpha\"\n",
    "guidance_scale = 3.5  # Keep guidance_scale at 3.5\n",
    "n_steps = 28\n",
    "seed = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = FluxPipeline.from_pretrained(model_id, torch_dtype=torch_dtype).to(device)\n",
    "pipe.save_pretrained(\"flux-lite\")\n",
    "\n",
    "def generate_image(prompt):\n",
    "    with torch.inference_mode():\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            generator=torch.Generator(device=\"cpu\").manual_seed(seed),\n",
    "            num_inference_steps=n_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            height=1024,\n",
    "            width=1024,\n",
    "        ).images[0]\n",
    "    image.save(prompt[:10] + \".png\")\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "    George, the curious brown monkey, found himself standing in front of a shiny red fire truck one sunny day.\n",
    "    He was in awe of the firefighters in their uniforms and helmets. \n",
    "    Behind them, the city buildings rose high, and a tree swayed gently in the breeze.\",\n",
    "    \"Later, George found himself in a messy kitchen.\n",
    "    He couldn't resist touching the underside of an overturned frying pan on the stovetop. \n",
    "    He wondered how it had ended up there amidst the scattered items\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a story teller, and whatever story you are given, split it into meaningful parts for image generation prompts\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": story},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=512)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storybook generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We take the segemnts generated by the diffusion model and feed them one by one into FLUX, then take the seqeunce of images and strign as one gif.\n",
    "- In v1, we will generate 4 versions of the image, or use latent interpolation to create a video-like effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = response.split(\".\\n\")\n",
    "print(prompt_list)\n",
    "\n",
    "image_seq = []\n",
    "\n",
    "for prompt in tqdm(prompt_list):\n",
    "    image = generate_image(prompt)\n",
    "    image_seq.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_grid(images, n_cols=4, figsize=(15, 15)):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      images: A list of PIL Image objects.\n",
    "      n_cols: The number of columns in the grid.\n",
    "      figsize: The figure size in inches.\n",
    "    \"\"\"\n",
    "\n",
    "    n_rows = len(images) // n_cols + (len(images) % n_cols > 0)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        row, col = i // n_cols, i % n_cols\n",
    "        axes[row, col].imshow(image)\n",
    "        axes[row, col].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "display_image_grid(image_seq)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
